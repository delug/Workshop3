{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Techniques and Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section of the tutorial, we'll gain an understanding for how to approach a **machine learning** problem and implement a benchmark machine learning algorithm. \n",
    "\n",
    "We'll start with a common dataset in the standard **attribute-predictor format**, and then venture into machine learning with **image data**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attribute-Predictor Data\n",
    "Let's first load our dataset. We'll look at the common **Breast Cancer dataset**, available in **ARFF** format. \n",
    "\n",
    "This dataset contains statistical information about **cell nuclei** as seen in digitized images from breast mass fine needle aspirates (FNA), a common type of biopsy. \n",
    "\n",
    "The information in each example either represents a group of benign or malignant cancer nuclei. \n",
    "\n",
    "Our **Goal** is to use machine learning to **recognize a relationship** between the statistical information (`data`) and the `label` of \"malignant\" or \"benign\", so we can *predict malignancy in new images* that do not have a label assigned to them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc = load_breast_cancer()\n",
    "print(bc.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to notice is that our data are organized into **attributes**. \n",
    "\n",
    "These can be thought of as *categorical properties* of the dataset, representing various **aspects** of the data. It looks like we have **10 properties**, each of which is represented by **3 statistical measures**: mean, standard error, and largest values. \n",
    "\n",
    "+ The **mean** offers the average of an attribute's values for a given instance, providing *summary* information. \n",
    "+ The **standard deviation** offers information about the *variance* of the attribute's values; \n",
    "    + i.e., how much do the values deviate from the mean. \n",
    "+ The **largest value** is a less common statistic, but seems fitting for cancer data, in which we expect cell nuclei of malignant cancer cells to be larger than benign cancer nuclei. \n",
    "\n",
    "Each **example** or **instance** of our data represents a separate fine needle aspirate with its own nuclei characteristics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_df = pd.DataFrame(bc.data, columns=bc.feature_names)\n",
    "bc_df.insert(len(bc_df.columns), 'target', bc.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration\n",
    "\n",
    "It's always a good idea to explore your data a bit before throwing it into an algorithm.\n",
    "\n",
    "This can help you find **anomalies** in the data, determine what **preprocessing** steps may be needed, choose which types of **algorithms** to use, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "hist = bc_df.hist(bins=15, figsize=(15,13))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histograms show the **distribution** of an attribute, and can allow us to form initial **hypotheses** about the data. \n",
    "\n",
    "Above, we see that some of the mean and \"worst\" attributes are **normally distributed**, and many of the \"error\" attributes show highest frequency around 0 and a tail to the right. Both of these are expected. \n",
    "\n",
    "We also see a tendency for **skewness** in the distributions toward the right. We might hypothesize that malignant nuclei tend to have larger means and larger \"worst\" statistics, and are thus responsible for the right skew."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be useful to compare the distributions of various attributes. For example, let's look at the **means**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = list([att for att in bc.feature_names if 'mean' in att])\n",
    "hist = bc_df[means].hist(bins=15, figsize=(12,12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the ranges for these attributes are not the same, so direct comparison is not feasible. Let's **standardize** the values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "x = bc_df.iloc[:,:len(bc_df.columns)-1].values #returns a numpy array\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "\n",
    "bc_df = pd.DataFrame(x_scaled, columns=bc.feature_names)\n",
    "bc_df.insert(len(bc_df.columns), 'target', bc.target)\n",
    "\n",
    "bc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = bc_df[['mean radius','mean area','mean concavity']].plot.hist(bins=15, alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relationship\n",
    "\n",
    "Next, let's look at the relationship between an attribute and the binary predictor. \n",
    "\n",
    "We see that `mean concavity` has the highest skewed histogram. Based on our hypothesis that skewness is due to malignancy, let's check the predictive value of this attribute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatt = bc_df.plot.scatter(x='mean concavity',y='target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that values below ~0.04 will most certainly be classified as `benign` (class label of `0` = malignant) and above 0.39 will likely be labeled `malignant`. \n",
    "\n",
    "There's still considerable overlap between 0.04 and 0.4, so let's see if information from *other attributes* helps us predict the right class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap = bc_df[(bc_df['mean concavity'] >= 0.04) & (bc_df['mean concavity'] <= 0.4)]\n",
    "scatt1 = overlap.plot.scatter(x='mean concavity', y='target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatt2 = overlap.plot.scatter(x='mean radius', y='target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now apparent that among the examples with `mean concavity` values between 0.04 and 0.4, if an example has `mean radius` of 0.25 or less, we will predict `benign`, and we will predict `malignant` for values above 0.4. \n",
    "\n",
    "Now, our uncertainty is reduced to examples with `mean concavity` between 0.04-0.4 *and* `mean radius` between 0.25-0.4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Our uncertainty based solely on Mean Concavity consists of {len(overlap)} values.\")\n",
    "\n",
    "overlap2 = overlap[(overlap['mean radius']>= 0.25) & (overlap['mean radius'] <= 0.4)]\n",
    "print(f\"And our uncertainty based on Mean Concavity AND Mean Radius consists of {len(overlap2)} values.\")\n",
    "\n",
    "print(f\"We've reduced our uncertainty by a factor of {len(overlap)/len(overlap2):.2f}!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this newfound confidence, let's design a **classification rule** that predicts `benign` if the values are below 0.35 and `malignant` if they're above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.scatter(overlap['mean radius'], overlap['target'])\n",
    "plt.axvline(x=0.35,color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, we've manually conducted the initial steps of a **decision tree**. \n",
    "\n",
    "Our decision tree works as follows: \n",
    "1. If `mean concavity` < 0.04: `target` = `benign`\n",
    "2. If `mean concavity` > 0.4: `target` = `malignant`\n",
    "3. If `mean radius` < 0.35: `target` = `benign`\n",
    "4. Otherwise, `target`=`malignant`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=[]\n",
    "for idx in range(len(bc_df['target'])):\n",
    "    if bc_df['mean concavity'][idx]<0.04: pred.append(1)\n",
    "    elif bc_df['mean concavity'][idx]>0.4: pred.append(0)\n",
    "    elif bc_df['mean radius'][idx]<0.35: pred.append(1)\n",
    "    else: pred.append(0)\n",
    "\n",
    "misclass=0\n",
    "for predxn in range(len(pred)):\n",
    "    if pred[predxn]!=bc_df['target'][predxn]: misclass+=1\n",
    "\n",
    "error = misclass/len(bc_df)*100\n",
    "\n",
    "print(f\"Our decision tree achieved {100-error:.2f}% accuracy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've just manually / conceptually performed a very basic algorithm for data analysis, similar to those used by machine learning alorithms. \n",
    "\n",
    "Our algorithm:\n",
    "1. We first performed a \"prediction\" using a single attribute. \n",
    "2. We (conceptually) saw that predicting based on this single attribute results in large uncertainty. \n",
    "    + Uncertainty is a source of **misclassifications**, or **error** in our predictions. \n",
    "3. We then **updated** our initial model, thereby improving our prediction. \n",
    "\n",
    "Machine learning algorithms attempt to **model some aspect(s) the data space** by making predictions, observing the error from those predictions, and updating the model to reduce the error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Machine Learning** involves the following elements:\n",
    "\n",
    "+ **Dataset**\n",
    "    + The Law of Large Numbers allows us to utilize a representative *sample* of our population, assuming the dataset is **i.i.d.**\n",
    "+ **Task**\n",
    "    + Our algorithm will be given a task to perform on this dataset\n",
    "    + e.g., **Classification** vs. **Regression**\n",
    "    + **Supervised** vs. **Unsupervised**\n",
    "+ **Model**\n",
    "    + We will choose a model to perform the task\n",
    "    + e.g., linear, non-linear, parametric, non-parametric\n",
    "+ **Loss Function**\n",
    "    + The loss function for our model allows us to compute the performance of our model and update its parameters accordingly\n",
    "    + e.g., squared error, 0-1 loss, cross-entropy\n",
    "+ **Training**\n",
    "    + Optimization, Solver, Cross Validation, etc. \n",
    "+ **Evaluation**\n",
    "    + Utilizing Performance measures\n",
    "    + e.g. F1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    "Our prediction above could be improved in two ways:\n",
    "1. We only use two of the 30 attributes. \n",
    "    + Manual analysis of more than 2-3 attributes at a time is very difficult\n",
    "2. We used a vertical line to separate the data.\n",
    "    + It is difficult to manually estimate what slope a line should be and where the center of mass in a group of data lies.\n",
    "\n",
    "**Linear regression** seeks to \"**fit**\" the data points so as to describe the target variable *as a function of* the data. I.e., as values of the data change along this line of best fit, the corresponding target value should change in a linear and predictable way. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall: ` y = mx + b `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This hallmark equation from grade school shows how a group of data points (`x`) can undergo simple linear combinations to produce an output (`y`). Here, `m` represents the **slope** and `b` is our **bias** term. \n",
    "\n",
    "In Statistics, we typically write this as $y = \\beta_{1} x + \\beta_{0}$. We express the output in terms of the **expectation** of **random variable Y *given* data X**, $\\mathop{\\mathbb{E}}[Y | X]$.\n",
    "\n",
    "( The bias represents information about our output `y` that the model `mx` cannot or does not predict, termed **uncontrollable error**. If there were no relationship between `y` and `x` (i.e., the **null hypothesis** $\\beta_0$), then we would set `m`=0 and use solely the bias term `b` to predict a constant value `y`=`b`. Typically, however, the bias helps account for noise in the data, which we do not want our model to predict. )\n",
    "\n",
    "We can extend the equation to predict many points **`y`** from a vector of inputs **`x`**. Linear regression finds the single value `m` that, when applied to each individual `x`, predicts an output `y`. This gives us a single value `m` that describes the entire dataset.  \n",
    "\n",
    "The output `y` is a vector of **probabilities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = bc_df.iloc[:int(len(bc_df)*0.8)]\n",
    "test = bc_df.iloc[int(len(bc_df)*0.8):]\n",
    "test_labels = test['target']\n",
    "test = test.drop('target',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression can be easily implimented using the wonderful `scikitlearn` library!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll first perform **Simple Linear Regression**; i.e., we'll only use a single attribute from the data and thus fit a single coefficient to that attribute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression().fit(train['mean concavity'].values.reshape(-1,1), train['target'].values.reshape(-1,1))\n",
    "m = reg.coef_\n",
    "b = reg.intercept_\n",
    "simple_pred = reg.predict(test['mean concavity'].values.reshape(-1,1))\n",
    "\n",
    "for sp in range(len(simple_pred)):\n",
    "    if simple_pred[sp]>=0.5: simple_pred[sp]=1\n",
    "    else: simple_pred[sp]=0\n",
    "\n",
    "print(f\"Our fitted coefficient equals {m[0][0]:.4f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclass=0\n",
    "for predxn in range(len(simple_pred)):\n",
    "    if simple_pred[predxn]!=test_labels.values[predxn]: misclass+=1\n",
    "\n",
    "error = misclass/len(test_labels)*100\n",
    "print(f\"Simple Linear Regression achieved {100-error:.2f}% accuracy.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.scatter(train['mean concavity'], train['target'])\n",
    "lobf = m * train['mean concavity'].values.reshape(-1,1) + b\n",
    "plt.plot(train['mean concavity'].values,lobf,color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple Linear Regression\n",
    "\n",
    "Linear regression is further extendable to predict outcomes `y` given any number of attributes. Our input vector **`x`** becomes our dataset matrix `X`, with a column for each attribute and a row for each instance. \n",
    "\n",
    "We now fit a `m` **coefficient** for *each* attribute, resulting in a **`w`** vector. Similarly, each attribute gets a bias term. \n",
    "\n",
    "$$\\vec y=X\\vec w$$\n",
    "\n",
    "\n",
    "$$X=\\begin{bmatrix}\n",
    "1 & x_{1,1} & x_{1,2} & \\cdots  & x_{1,n}\\\\ \n",
    "1 & x_{2,1} & \\ddots &  & \\vdots\\\\\n",
    "\\vdots &  \\vdots &  &  \\ddots& \\vdots\\\\\n",
    "1 &  x_{m,1}& \\cdots &  \\cdots& x_{m,n}\\\\ \n",
    "\\end{bmatrix}$$\n",
    "\n",
    "$$\\vec y=X\\vec w\\implies \\vec w= X^{-1}\\vec y \\text{  iff  } X^{-1} \\text{  exists}$$\n",
    "Instead, we can rely on a special matrix $X^\\intercal X$ which is guaranteed to have an inverse so long as the columns of X are linearly independent. So then,\n",
    "\n",
    "$$\\vec y=X\\vec w\\implies X^\\intercal\\vec y=(X^\\intercal X)\\vec w\\implies (X^\\intercal X)^{-1}X^\\intercal\\vec y=(X^\\intercal X)^{-1}(X^\\intercal X)\\vec w=I\\vec w=\\vec w$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mult_reg = LinearRegression().fit(train.iloc[:,:-1].values, train['target'].values)\n",
    "mult_pred = mult_reg.predict(test.values)\n",
    "\n",
    "for mp in range(len(mult_pred)):\n",
    "    if mult_pred[mp]>=0.5: mult_pred[mp]=1\n",
    "    else: mult_pred[mp]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclass=0\n",
    "for predxn in range(len(mult_pred)):\n",
    "    if mult_pred[predxn]!=test_labels.values[predxn]: misclass+=1\n",
    "\n",
    "error = misclass/len(test_labels)*100\n",
    "print(f\"Multiple Linear Regression achieved {100-error:.2f}% accuracy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "There are some limitations with Linear Regression. As can be seen in the plot above, the result of linear regression is a **line** of best fit with range -$\\infty$ to $\\infty$. \n",
    "\n",
    "However, since we are dealing with **classification**, we only want predictions at 0 or 1. \n",
    "\n",
    "It is difficult to interpret what a prediction less than 0 or greater than 1 means. \n",
    "\n",
    "Additionally, we must arbitrarily set a **threshold** between 0 and 1, above which all predictions become `1` and below which all predictions become `0`. If many predictions are near the threshold, the potential for error is high. \n",
    "\n",
    "As such, it is desirable to define the line of best fit as a **sigmoid** curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" source: http://squall0032.tumblr.com/post/77300791096/plotting-a-sigmoid-function-using \"\"\"\n",
    "import math\n",
    "\n",
    "def sigmoid(x):\n",
    "    a = []\n",
    "    for item in x:\n",
    "        a.append(1/(1+math.exp(-item)))\n",
    "    return a\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = np.arange(-10., 10., 0.2)\n",
    "sig = sigmoid(x)\n",
    "plt.plot(x,sig)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the sigmoid curve lies in the range [0,1] and concentrates most of its values around 0 and 1. The \"s\"-like curvature keeps the function continuous, meaning it is differentiable. We'll return to this point in the next section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, the equation for the sigmoid function is $\\sigma (x)=\\frac{1}{1+e^{-x}}$. Given a Bernoulli random variable $Y$ with only two classes, e.g., $1$ and $0$, we can write $p(x) = \\mathop{\\mathbb{P}}[Y=1 | X=x]$. So by our model, we assert that $$p(x)=\\sigma (x)=\\frac{1}{1+e^{-x}}$$, but we'd like to parameterize this as $$\\frac{1}{1+e^{-(\\beta^Tx + \\beta_{0})}}$$. In order to do so, we need to solve for $(\\beta^Tx + \\beta_{0})$, or if we simplify it as $x'=(\\beta^Tx + \\beta_{0})$, then we only need to solve for $x'$ in $p(x')$.\n",
    "\n",
    "$$p(x')=\\frac{1}{1+e^{-x'}}\\implies\\frac{1}{p(x')}=1+e^{-x}\\implies\\frac{1}{p(x')}-1=e^{-x}\\implies\\frac{1-p(x')}{p(x')}=e^{-x}\\implies\\frac{p(x')}{1-p(x')}=e^x\\implies \\operatorname{ln}(\\frac{p(x')}{1-p(x')})=x$$\n",
    "\n",
    "So finally, we have our model: $$p(x)=\\frac{1}{1+e^{-(\\beta^Tx + \\beta_{0})}}$$\n",
    "Now that we have our new model for our data, we aim to fit this model by tuning the coefficients $\\beta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression().fit(train.iloc[:,:-1].values, train['target'].values)\n",
    "logr_pred = logreg.predict(test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclass=0\n",
    "for predxn in range(len(logr_pred)):\n",
    "    if logr_pred[predxn]!=test_labels.values[predxn]: misclass+=1\n",
    "\n",
    "error = misclass/len(test_labels)*100\n",
    "print(f\"Logistic Regression achieved {100-error:.2f}% accuracy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "\n",
    "We've seen two examples of *modeling* the data, as either a straight line or a sigmoid curve. However, both of these models involve **parameters** that need to be solved to make the models **fit** the data. \n",
    "\n",
    "There are infinite possible straight lines or sigmoid curve variations we could potentially model the data with. We need to find the *best fit* in an efficient way... \n",
    "\n",
    "**Optimization** is the process of **maximizing** some parameterized **function** based on a **metric** (or **minimizing** based on its negative).\n",
    "\n",
    "##### Which objective function are we minimizing?\n",
    "\n",
    "For the case of **linear regression**, it is simple to look at the *number of misclassified examples*, or the **error** on our predections.\n",
    "\n",
    "More specifically, the L2 loss for linear regression, aka the **least squares equation**: \n",
    "\n",
    "<p style=\"text-align: center;\"> $l(x_{i}, y_{i}) = \\|(y_{i} - (\\beta x_{i} + \\beta_{0})\\|^2$ </p>\n",
    "\n",
    "When dealing with the entire dataset, we'll combine $\\beta_{0}$ with vector $\\beta$ and add a dummy column of 1's to X:\n",
    "\n",
    "<p style=\"text-align: center;\"> $l(X, y) = \\|y - \\beta^T X\\|^2$ </p>\n",
    "\n",
    "**Logistic regression** has some interesting properties that let's us minimize its function more directly. \n",
    "\n",
    "If we suppose `y` to take the values -1 and 1 instead of 0 and 1, we can represent `y` as follows: \n",
    "\n",
    "<p style=\"text-align: center;\"> $\\mathop{\\mathbb{P}}[Y=1 | X=x] = \\frac{1}{1 + e^{-(\\beta^TX)}}$ and $\\mathop{\\mathbb{P}}[Y=-1 | X=x] = \\frac{1}{1 + e^{(\\beta^TX)}}$ </p>\n",
    "\n",
    "This can be simplified to: $\\mathop{\\mathbb{P}}[Y=y | X=x] = \\frac{1}{1 + e^{(-y \\beta^TX)}} = \\sigma(-y \\beta^TX)$\n",
    "\n",
    "For each class, we seek to **maximize** the probability that Y=y given the data, by fitting our parameters $\\beta$ to the data. This is referred to as the **maximum likelihood equation**: \n",
    "\n",
    "<p style=\"text-align: center;\"> ${\\displaystyle \\max_{\\beta \\in \\Theta}} {\\mathop{\\mathcal{L}}}(\\beta;X)$\n",
    "\n",
    "where $\\Theta$ represents the parameter space of all possible parameter values.\n",
    "\n",
    "This is solved as \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  {\\mathop{\\mathcal{L}}}(\\beta;X) &= p((x_{1},y_{1}),(x_{2},y_{2}),...,(x_{n},y_{n});\\beta) \\\\\n",
    " &= {\\displaystyle \\prod_{i=1}^{n} p(x_{i},y_{i}; \\beta)} \\\\\n",
    " &= {\\displaystyle \\prod_{i=1}^{n} {\\rm p}^{y_{i}}(1-{\\rm p})^{(1-y_{i})}},\n",
    "\\end{aligned}\n",
    "$$\n",
    "where ${\\rm p} = \\sigma(y_{i}\\beta^TX)$ and $y_{i}$ is a **Bernoulli** variable.\n",
    "\n",
    "The **log likelihood function** is more convenient to use:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\log {\\mathop{\\mathcal{L}}}(\\beta;X) &= \\log {\\displaystyle \\prod_{i=1}^{n} {\\rm p}^{y_{i}}(1-{\\rm p})^{(1-y_{i})}} \\\\\n",
    "&= {\\displaystyle \\sum_{i=1}^{n} y_{i} \\log {\\rm p} + (1 - y_{i}) \\log(1- {\\rm p}) }\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This equation gives us the **cross entropy**, equivalent to the **log likelihood**\n",
    "\n",
    "##### How can we update our parameters so as to minimize the error?\n",
    "\n",
    "Recall: `gradient` \n",
    " \n",
    "+ Represents the line tangent to a function at a given point; i.e., the instantaneous **slope** of the line\n",
    "\n",
    "+ At inflection points, the gradient = 0\n",
    "\n",
    "Given that our function is **differentiable**, the loss function is also differentiable. \n",
    "\n",
    "Therefore, taking the gradient of the loss function will tell us the slope of our **error curve** given the input parameters. \n",
    "\n",
    " $$\\nabla\\operatorname{log}\\mathcal{L}(\\beta ; X)=\\frac{1}{n}\\sum_{i=1}^n(\\sigma (y_i\\beta ^\\intercal x_i)-1)y_ix_i$$\n",
    "\n",
    "#### Gradient Descent\n",
    "\n",
    "The negative of this gradient tells us the direction toward a (at least local) **minima**.\n",
    "\n",
    "Thus, we can update our parameters as follows:\n",
    "\n",
    "1. Initialize $\\beta^{(0)}$\n",
    "2. $\\beta^{(k+1)} = \\beta^{(k)} - \\alpha_k \\nabla l(\\beta^{(k)}) $\n",
    "    + $\\alpha_k$ is a **learning rate** used to weight the effect of the gradient on the update\n",
    "    + larger update steps can be made at the beginning of training; closer to the optimum, more refined updates should be made\n",
    "3. Repeat until convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.oreilly.com/library/view/learn-arcore-/9781788830409/assets/f3899ca3-835e-4d3e-8e7f-fd1c5a9044fb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.stack.imgur.com/gjDzm.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Switching Domains : Image Data\n",
    "\n",
    "Now that we have an understanding of basic machine learning, we can switch to more complex, but interesting domain. \n",
    "\n",
    "**Imaging data** can be viewed just like normal datasets, where a single example is made up of n **pixels**, and each pixel represents a distinct attribute. \n",
    "\n",
    "Across all the example, each pixel's values take a distinct distribution, and all the pixels' distributions together make up the **data space**. \n",
    "\n",
    "Next, we'll apply the machine learning algorithms we've just seen as well as some new ones to the common **MNIST** image dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources: \n",
    "+ The contents of this tutorial were done largely from memory of lessons over the years and refreshers from the internet, as well as with guidance from slides by Dr. Jaewoo Lee (CSCI 8960, UGA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is K-Nearest Neighbors? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors is a non-parametric, lazy learning algorithm. K-Nearest Neighbors seperates data into several classes in order to predict the classification of a new sample point. By __non-parametric__, we mean that it makes no assumptions about the underlying data distribution, or in other words that the model structure is determined from the data. KNN is a __lazy learning algorithm__ because there is no explicit training phase, and that its training data is needed during the testing phase in order for KNN to generalize. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/knn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN relies on feature similarity, or how closely out-of-sample features resemble our training set. In the above picture, the new example will be classified as either a red triangle or blue square depending on how many nearest neighbors (k) we set. \n",
    "\n",
    "When k equals 1, the new example is classified to a blue square since the nearest neighbor is a blue square.\n",
    "\n",
    "When k equals 3, the new example is classified to a red triangle since there are two red triangles and only one blue square.\n",
    "\n",
    "When k equals 5, the circle is now enclosed to the first 5 datapoints that it reaches, which in this case will be 3 squares and 2 triangles. This means that the example will be classified as a square."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Applied to Digit Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll import all necessary models and then load the MNIST dataset. We're primarily concerned with the ```mnist.data``` and ```mnist.target``` columns. ```mnist.data``` contains 784 columns for each pixel of a 28x28 pixel image, in which each element represents a greyscale codes from 0 to 255 at that specific pixel. ```mnist.target``` represents labels for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "mnist = fetch_openml('mnist_784', version=1, cache=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is MNIST?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mnist.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_df = pd.DataFrame(mnist.data, columns=mnist.feature_names)\n",
    "mnist_df.insert(len(mnist_df.columns), 'target', mnist.target)\n",
    "mnist_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mnist_df['target'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll separate the arrays and the classes. The shape of x is 70000 by 784, meaning that there are 784 columns (one for each pixel of the 28x28 image) and 70000 rows these arrays. The shape of y is 70000 rows of numbers from 0 to 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mnist[\"data\"], mnist[\"target\"] \n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can mess around with matplotlib to visually give us an idea of what the array's greyscale numbers mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "some_digit = X[9]\n",
    "some_digit_image = some_digit.reshape(28,28)\n",
    "fig=plt.figure(figsize=(8, 8))\n",
    "digits = X[np.random.randint(0, high=70000, size=10)]\n",
    "rows = 2\n",
    "columns = 5\n",
    "\n",
    "for i in range(len(digits)):\n",
    "    fig.add_subplot(rows, columns, i+1)\n",
    "    plt.imshow(digits[i].reshape(28,28), cmap=matplotlib.cm.binary,\n",
    "              interpolation=\"nearest\")\n",
    "    plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Data Visualized "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before diving into the visualization of our data, let's learn what __dimensionality reduction__ is. \n",
    "\n",
    "__Dimensionality reduction__, put simply, is the process of reducing the dimensions of a feature set. A dataset with 100 features could be condensed to 20 meaningful features by dimensionality reduction. Who cares if we have 100 features? _What's wrong with having more features?_ More than one would think. A model that trains on numerous features becomes increasingly dependent on the data it was trained on and overfits as a result. Some of the motivations for dimensionality reduction include:\n",
    "\n",
    " - Avoiding overfitting\n",
    " - Remove redundant features\n",
    " - Improve computation time\n",
    " - Visualization\n",
    " \n",
    " and more!\n",
    "\n",
    "In a dataset such as MNIST, we use dimensionality reduction strictly for visualization purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's clearly infeasible to try visualizing our data since it's in a dimensionality we couldn't begin to fathom how to visualize. Our solution to this is __dimensionality reduction__. We can reduce the data from 784 dimensions to 2 dimensions using __t-Distributed Neighbor Stochastic Embedding__, or __t-SNE__ for short. t-SNE is an unsupervised, non-linear technique used for data exploration and visualizing high-dimensional data. In short, the way t-SNE works is by calculating a probabilistic similarity measure between instance pairs in both the high dimensionality space and low dimensionality space. It then optimizes these similarity measures using a cost function. Compared to __Principal Components Analysis__ (PCA), a linear dimensionality reduction technique, t-SNE can be used to better visualize nonlinear datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/swissroll.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solid line in the above graph represents t-SNE which preserves small pairwise distances or local similarities. The dotted line represents PCA which preserves large pairwise distances to maximize variance. What we can see from this is that PCA does not preserve the underlying structure of the dataset while t-SNE does. More information on the mathematics behind t-SNE will be linked below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2).fit_transform(X[:7000])\n",
    "\n",
    "reduced_df = np.vstack((tsne.T, y[:7000])).T\n",
    "reduced_df = pd.DataFrame(data=reduced_df, columns=[\"X\", \"Y\", \"label\"])\n",
    "reduced_df.label = reduced_df.label.astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(reduced_df, hue='label', size=8).map(plt.scatter, 'X', 'Y').add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we can gather from this visualization are a couple things. For the most part, the digits are clustered separately quite well. Of course, there are some digits that fall into other clusters where they don't belong and this can be attributed to the poor handwriting of a certain digit (an 8 can look like a 6). Notably, there isn't much separation between 4s (purple) and 9s (teal) which shouldn't come as a big surprising considering the written similarity between a 4 and a 9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Our Train/Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are more familiar with our data, we can separate it into training and test sets. There are three datasets that a data scientist will generally interact with: a training set, a validation set, and a test set. We are primarily concerned with separating our data into a test and train split since we'll be using cross validation later. So what do each of these sets mean? The __training set__ is the sample of data that we fit our model to. Our model _learns_ from this data. The __validation set__ is the sample of data we use to evaluate our model on to fine-tune our __hyperparameters__, which are the parameters of the model that we are able to explicitly change. Hyperparameters are not to be confused with __model parameters__, which are parameters that the ML algorithm will learn on its own, such as weights and biases in a linear regression model or split points in a decision tree. Finally, the __test set__ is the sample of data we wish to test our model on to provide an unbiased evaluation of our model fit on the training data. The model has _never_ seen the test set data before, and an important note is that our test set should be stratified in the same way our training set is set up. It should contain a similar proportion of classes (digits 0 through 9) as our training set. A good rule of thumb is to separate our data into 80% train and 20% test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/traintest.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train and y_train will be our training set consisting of 60000 instances, while X_test and y_test will be the remaining 10000. We'll then use np.random.permutation to shuffle the indices so that the numbers are not in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_index = np.random.permutation(60000)\n",
    "X_train, y_train = X_train[shuffle_index], y_train[shuffle_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training is as simple as creating a knn classifier with three neighbors required to classify and then calling knn.fit() to fit the classifier to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Sure Our Model Generalizes Well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get an idea of how our model will generalize by using __k-fold cross validation__ with 5 folds (generally 10 folds is recommended, but for sake of compiling time we've reduced it to 5). Cross validation can be used to attain an understanding of how well our model will generalize to unseen data. We want to make sure that our model is low on bias and variance, or in other words is not respectively underfitting or overfitting but providing an overall good fit. We use cross validation because we want to avoid the problem of reducing the amount of training data since this risks losing important patterns/trends in the dataset, which would lead to a higher error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-fold cross validation works by dividing the training dataset into k partitions, or folds. Each of these folds are used as a validation set once and the other k-1 sets are used to form the training set, so by the end of cross validation the training set has been validated on k sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/crossval.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_An example of how 5-fold cross validation works iteratively_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cvs = cross_val_score(knn, X_train, y_train, cv=5, scoring=\"accuracy\")\n",
    "#print(cvs)\n",
    "#print(np.average(cvs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can test our model on the test set and see our accuracy. 97%, not bad!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the Optimal Hyperparameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With each model data scientists use to solve a problem there exists a huge hyperparameter search space. It would be quite time consuming to hand-tune our hyperparemeters over and over again. This is where __grid search__ comes in, allowing us to search the hyperparameter space in a brute force fashion for the optimal hyperparameters. By the nature of grid search however, it is computationally complex on large datasets with large hyperparameter spaces. Therefore running grid search would be best done on a stratified subset of the training data. For the sake of time, we'll omit running the below code during the workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# instantiate the grid\n",
    "k_range = list(range(1, 5))\n",
    "param_grid = dict(n_neighbors=k_range)\n",
    "grid = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy')\n",
    "#grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Support Vector Machine? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Support Vector Machine, or SVM, is a supervised learning algorithm that can be used for either classification or regression. The algorithm outputs a hyperplane which categorizes new examples. Given two classes, a SVM will find the hyperplane that maximizes the margin, or distance, between the nearest points of the two classes. This hyperplane is called the __maximum-margin hyperplane__. SVMs are ideal for high-dimensional data, are resilient to overfitting, and are very versatile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/svm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Kernel Trick "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the hyperplanes above maximize the distance between the two classes, it's often the case that our data is not linearly separable, meaning that we cannot simply divide our classes with a linear hyperplane. For cases like this, we may translate our points into a higher dimensional space using the __kernel trick__ so that we may find a clear dividing margin. There are different kernels at our disposal, such as the polynomial kernel, the radial basis function kernel, and the sigmoid kernel to name a few."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/kernel.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Applied to Digit Recognition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this plethora of information on SVMs, we can start by standardizing our data and shuffling it like before. Due to the computational complexity of a polynomial kernel SVM, we'll be working with 10000 training samples and 2000 test samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = X_scaled[:10000], X_scaled[68000:], y[:10000], y[68000:]\n",
    "shuffle_index = np.random.permutation(10000)\n",
    "X_train, y_train = X_train[shuffle_index], y_train[shuffle_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating and fitting our model is as simple as two lines of code! We should pay attention to the parameters of the support vector classifier. We are using a polynomial kernel, and our C value will denote how narrow or wide we want our margin to be. A _smaller_ C value will give us a _larger_ margin, whereas a _bigger_ C value will give us a _smaller_ margin. To elaborate, C is the sensitivity of the SVM algorithm in relation to how lenient we want the algorithm to be in misclassification. A wider margin will mean a higher likelihood of misclassification while a narrow margin will mean a lower likelihood. Gamma represents a hyperparameter that determines how much a new data point influences the curvature of our decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='poly', C=.001, gamma=10)\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Confusion Matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know what our accuracy is now, but we want to get an idea now of how our model performed on classifying different digits. The easiest way to see our performance is by creating a __confusion matrix__. This will allow us to see how our model performed on predicting digits versus the actual values. The diagonal of the matrix represents the correct classifications and all other cells represent misclassifications. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_true, y_pred).astype('int')]\n",
    "    #if normalize:\n",
    "    #    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    #    print(\"Normalized confusion matrix\")\n",
    "    #else:\n",
    "    #    print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8,8))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    #fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "\n",
    "plot_confusion_matrix(y_test, y_pred, classes=np.unique(mnist['target']), title=\"Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the confusion matrix that the model performed very well on the 1 digit. The highest misclassification happened with 8s being predicted as 5s.  Similarly, six 2s were classified as 8s somehow. We can also gather from this confusion matrix that our subset sample had a lot of 1s and not a lot of 5s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Reading "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality reduction: https://www.geeksforgeeks.org/dimensionality-reduction/\n",
    "\n",
    "t-SNE: https://towardsdatascience.com/an-introduction-to-t-sne-with-python-example-5a3a293108d1\n",
    "\n",
    "SVM: https://medium.com/machine-learning-101/chapter-2-svm-support-vector-machine-theory-f0812effc72"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
