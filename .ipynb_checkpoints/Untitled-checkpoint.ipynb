{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, the equation for the sigmoid function is $\\sigma (x)=\\frac{1}{1+e^{-x}}$. Given a Bernoulli random variable $Y$ with only two classes, e.g., $1$ and $0$, we can write $p(x) = \\mathop{\\mathbb{P}}[Y=1 | X=x]$. So by our model, we assert that $$p(x)=\\sigma (x)=\\frac{1}{1+e^{-x}}$$, but we'd like to parameterize this as $$\\frac{1}{1+e^{-(\\beta^Tx + \\beta_{0})}}$$. In order to do so, we need to solve for $(\\beta^Tx + \\beta_{0})$, or if we simplify it as $x'=(\\beta^Tx + \\beta_{0})$, then we only need to solve for $x'$ in $p(x')$.\n",
    "\n",
    "$$p(x')=\\frac{1}{1+e^{-x'}}\\implies\\frac{1}{p(x')}=1+e^{-x}\\implies\\frac{1}{p(x')}-1=e^{-x}\\implies\\frac{1-p(x')}{p(x')}=e^{-x}\\implies\\frac{p(x')}{1-p(x')}=e^x\\implies \\operatorname{ln}(\\frac{p(x')}{1-p(x')})=x$$\n",
    "\n",
    "So finally, we have our model: $$p(x)=\\frac{1}{1+e^{-(\\beta^Tx + \\beta_{0})}}$$\n",
    "Now that we have our new model for our data, we aim to fit this model by tuning the coefficients $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "\n",
    "We've seen two examples of *modeling* the data, as either a straight line or a sigmoid curve. However, both of these models involve **parameters** that need to be solved to make the models **fit** the data. \n",
    "\n",
    "There are infinite possible straight lines or sigmoid curve variations we could potentially model the data with. We need to find the *best fit* in an efficient way... \n",
    "\n",
    "**Optimization** is the process of **maximizing** some parameterized **function** based on a **metric** (or **minimizing** based on its negative)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Which objective function are we minimizing?\n",
    "\n",
    "For the case of **linear regression**, it is simple to look at the *number of misclassified examples*, or the **error** on our predections.\n",
    "\n",
    "More specifically, the L2 loss for linear regression, aka the **least squares equation**: \n",
    "\n",
    "<p style=\"text-align: center;\"> $l(x_{i}, y_{i}) = \\|(y_{i} - (\\beta x_{i} + \\beta_{0})\\|^2$ </p>\n",
    "\n",
    "When dealing with the entire dataset, we'll combine $\\beta_{0}$ with vector $\\beta$ and add a dummy column of 1's to X:\n",
    "\n",
    "<p style=\"text-align: center;\"> $l(X, y) = \\|y - \\beta^T X\\|^2$ </p>\n",
    "\n",
    "**Logistic regression** has some interesting properties that let's us minimize its function more directly. \n",
    "\n",
    "If we suppose `y` to take the values -1 and 1 instead of 0 and 1, we can represent `y` as follows: \n",
    "\n",
    "<p style=\"text-align: center;\"> $\\mathop{\\mathbb{P}}[Y=1 | X=x] = \\frac{1}{1 + e^{-(\\beta^TX)}}$ and $\\mathop{\\mathbb{P}}[Y=-1 | X=x] = \\frac{1}{1 + e^{(\\beta^TX)}}$ </p>\n",
    "\n",
    "This can be simplified to: $\\mathop{\\mathbb{P}}[Y=y | X=x] = \\frac{1}{1 + e^{(-y \\beta^TX)}} = \\sigma(-y \\beta^TX)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each class, we seek to **maximize** the probability that Y=y given the data, by fitting our parameters $\\beta$ to the data. This is referred to as the **maximum likelihood equation**: \n",
    "\n",
    "<p style=\"text-align: center;\"> ${\\displaystyle \\max_{\\beta \\in \\Theta}} {\\mathop{\\mathcal{L}}}(\\beta;X)$\n",
    "\n",
    "where $\\Theta$ represents the parameter space of all possible parameter values.\n",
    "\n",
    "This is solved as \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  {\\mathop{\\mathcal{L}}}(\\beta;X) &= p((x_{1},y_{1}),(x_{2},y_{2}),...,(x_{n},y_{n});\\beta) \\\\\n",
    " &= {\\displaystyle \\prod_{i=1}^{n} p(x_{i},y_{i}; \\beta)} \\\\\n",
    " &= {\\displaystyle \\prod_{i=1}^{n} {\\rm p}^{y_{i}}(1-{\\rm p})^{(1-y_{i})}},\n",
    "\\end{aligned}\n",
    "$$\n",
    "where ${\\rm p} = \\sigma(y_{i}\\beta^TX)$ and $y_{i}$ is a **Bernoulli** variable.\n",
    "\n",
    "The **log likelihood function** is more convenient to use:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\log {\\mathop{\\mathcal{L}}}(\\beta;X) &= \\log {\\displaystyle \\prod_{i=1}^{n} {\\rm p}^{y_{i}}(1-{\\rm p})^{(1-y_{i})}} \\\\\n",
    "&= {\\displaystyle \\sum_{i=1}^{n} y_{i} \\log {\\rm p} + (1 - y_{i}) \\log(1- {\\rm p}) }\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This equation gives us the **cross entropy**, equivalent to the **log likelihood**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
